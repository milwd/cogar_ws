.. _index:

Autonomous Waiters – Complete Project Documentation
===================================================

Why this project?
-----------------
Busy sushi restaurants lose revenue when diners wait too long or receive the
wrong dish.  
Your solution deploys **five PAL-Robotics TIAGo robots** that handle order
delivery, table clearing and basic customer interaction – freeing staff for
tasks that need a human touch. :contentReference[oaicite:0]{index=0}&#8203;:contentReference[oaicite:1]{index=1}

Key goals
~~~~~~~~~
* **Timely delivery** – select the closest available robot, plan an
  obstacle-free path and adapt on the fly.  
* **Error-free serving** – detect mismatched orders and handle disputes
  gracefully.  
* **Scalability** – a central orchestrator must coordinate multiple robots
  without bottlenecks.  
* **Recoverability** – every component is restart-safe; no single point of
  failure.

Hardware platform
-----------------
* TIAGo base with differential drive  
* 7-DoF arm + parallel gripper  
* RGB-D camera, LiDAR, SONAR, force sensor, microphones, speakers

Software architecture
---------------------
The stack follows a classic *perception → reasoning → actuation* pipeline plus
a thin orchestration layer that balances the fleet.

.. mermaid::

   graph TD
       A[Orchestration<br>(task_manager.py,<br>orchestration_and_coordination.py)]
       B[Perception<br>(camera.py …)]
       C[Reasoning<br>(path_planning.py,<br>reasoning_action.py …)]
       D[Actuation<br>(control_wheel.py,<br>control_arm.py,<br>control_gripper.py)]
       E[Feedback<br>(encoder_* , force.py ,<br>speech_generator.py)]
       A -->|verified order| C
       C -->|path + table cmd| D
       D -->|status| A
       D -->|raw encoders| E
       E -->|status| C

Codebase tour
-------------
Below you’ll find **every script** you wrote, grouped by the component layer
they belong to.  Each bullet links to the detailed API page generated by
*autodoc*.

Vision
~~~~~~~~~~
* `camera.py` – static RGB + synthetic depth generator  
* `camera_preprocessing.py` – colour correction & ROI cropping  
* `lidar.py`, `sonar.py` – 2-D obstacle clouds  
* `object_detection.py` – YOLO-v8 wrapper for plate and hand detection  
* `sensor_fusion.py` – fuse RGB-D, LiDAR and SONAR into a cost map  
* `microphone_node.py` – raw microphone frames  
* `distance_estimation.py` – depth-based size → distance converter

Reasoning & Planning
~~~~~~~~~~~~~~~~~~~~
* `path_planning.py` – A*/D* hybrid global planner  
* `reasoning_action.py` – turns symbolic commands into ActionLib goals  
* `reasoning_order_verification.py` – parses *“Can I have …”* utterances  
* `reasoning_table_placement.py` – finds a free, stable spot for the dish  
* `slam.py` – RTAB-Map wrapper for map building

Actuation & Low-level Control
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
* `control_wheel.py` – proportional wheel controller (base)  
* `control_arm.py` – ActionLib server for joint-space moves  
* `control_gripper.py` – open/close with timeout + feedback  
* `encoder_wheel.py`, `encoder_arm.py`, `encoder_gripper.py` – synthetic
  encoders for CI

Interaction
~~~~~~~~~~~
* `speech_generator.py` – buffers sentences and drips them to TTS  
* `voice_recognition.py` – integer → phrase stub ASR  
* `speaker.py` – last-mile relay to the loudspeaker

Orchestration & Management
~~~~~~~~~~~~~~~~~~~~~~~~~~
* `orchestration_and_coordination.py` – YAML-backed order queue + state
  gateway  
* `task_manager.py` – event aggregator, calls `robot_state_decision` to fetch
  the next task

Data-flow example
-----------------
#. `voice_recognition.py` publishes *“Can I have sushi”*  
#. `reasoning_order_verification.py` extracts **sushi** and calls
   `send_order` → queue stores the order  
#. A **Free** robot pings `/robot_state_decision` and receives the sushi task  
#. `reasoning_action.py` converts the plan into  
   `control_wheel` → `control_arm` → `control_gripper` goals  
#. On success `control_*` nodes emit *ARM_DONE*, *GRIPPER_DONE*, … which the
   `task_manager` feeds back into the orchestrator.

Development & testing
---------------------
* **Sim-only CI** – synthetic encoders (`encoder_*`) and the fake force sensor
  (`force.py`) let you run unit tests on GitHub Actions with zero hardware.  
* **Recording sessions** – use `camera_preprocessing.py --record` to save
  synchronised RGB-D bags for offline ML training.  
* **Hot reload** – every ActionLib server respects pre-emption; launch filess
  can restart individual components without bringing down the whole stack.

Running the system
------------------
.. code-block:: bash

   # one-liner that launches Gazebo, RViz and every node
   roslaunch cogar_ws sushi_fleet.launch

   # manual: bring up orchestrator, then a single robot instance
   rosrun cogar_ws orchestration_and_coordination.py &
   roslaunch cogar_ws tiago_bringup.launch robot_id:=1

.. warning::
   A map must be available in *$(rospack find cogar_ws)/maps/*.  
   Run `slam.py --save` the first time on a new restaurant layout.

Further reading
---------------
* Component diagrams, behavioural models and KPIs are documented under each
  module page.  
* The assignment brief (slides 13–15) is embedded in *docs/assets* for quick
  reference. :contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}

.. rubric:: Navigate the code

.. toctree::
   :maxdepth: 1
   :caption: Contents

   vision
   vision/vision_modules/camera
   vision/vision_modules/camera_preprocessing
   vision/vision_modules/object_detection
   vision/vision_modules/distance_estimation
   brain
   brain/brain_modules/reasoning_action
   brain/brain_modules/reasoning_table_placement
   brain/brain_modules/task_manager
   brain/brain_modules/reasoning_speech_generation
   navigation
   navigation/navigation_modules/sonar
   navigation/navigation_modules/lidar
   navigation/navigation_modules/sensor_fusion
   navigation/navigation_modules/slam
   navigation/navigation_modules/path_planning
   control
   control/control_modules/control_wheel
   control/control_modules/control_arm
   control/control_modules/control_gripper
   control/control_modules/encoder_wheel
   control/control_modules/encoder_arm
   control/control_modules/encoder_gripper
   control/control_modules/force
   interaction
   interaction/interaction_modules/voice_recognition
   interaction/interaction_modules/microphone
   interaction/interaction_modules/reasoning_order_verification
   interaction/interaction_modules/speaker
   server
   server/server_modules/orchestration_and_coordination